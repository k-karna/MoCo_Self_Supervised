{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TOeCqEZRB1FF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 1046, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\konar\\AppData\\Local\\Temp\\ipykernel_19080\\143493276.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Python39\\lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Python39\\lib\\site-packages\\torch\\functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Python39\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "C:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "C5kdgLdsn4iz",
    "outputId": "4376e5b7-2d8e-4ecb-e018-af852a6464ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m train_set \u001b[38;5;241m=\u001b[39m STL10(root\u001b[38;5;241m=\u001b[39mroot, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m unlabeled_set \u001b[38;5;241m=\u001b[39m STL10(root\u001b[38;5;241m=\u001b[39mroot, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munlabeled\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m test_set \u001b[38;5;241m=\u001b[39m \u001b[43mSTL10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torchvision\\datasets\\stl10.py:60\u001b[0m, in \u001b[0;36mSTL10.__init__\u001b[1;34m(self, root, split, folds, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_folds(folds)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torchvision\\datasets\\stl10.py:155\u001b[0m, in \u001b[0;36mSTL10.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torchvision\\datasets\\stl10.py:150\u001b[0m, in \u001b[0;36mSTL10._check_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_list \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_list:\n\u001b[0;32m    149\u001b[0m     fpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_folder, filename)\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:74\u001b[0m, in \u001b[0;36mcheck_integrity\u001b[1;34m(fpath, md5)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m md5 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_md5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:66\u001b[0m, in \u001b[0;36mcheck_md5\u001b[1;34m(fpath, md5, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_md5\u001b[39m(fpath: \u001b[38;5;28mstr\u001b[39m, md5: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m md5 \u001b[38;5;241m==\u001b[39m calculate_md5(fpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:61\u001b[0m, in \u001b[0;36mcalculate_md5\u001b[1;34m(fpath, chunk_size)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(chunk_size):\n\u001b[1;32m---> 61\u001b[0m         \u001b[43mmd5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m md5\u001b[38;5;241m.\u001b[39mhexdigest()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Getting dataset\n",
    "\n",
    "root = \"./data\"\n",
    "train_set = STL10(root=root, split=\"train\", download=True, transform=None)\n",
    "unlabeled_set = STL10(root=root, split='unlabeled', download=True, transform=None)\n",
    "test_set = STL10(root=root, split=\"test\", download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MY9br-xoaqZ"
   },
   "outputs": [],
   "source": [
    "def show_images(images, titles=None, nrows=2, ncols=5, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Display a grid of images.\n",
    "\n",
    "    Args:\n",
    "        images (list): List of images to display.\n",
    "        titles (list): List of titles for each image.\n",
    "        nrows (int): Number of rows in the grid.\n",
    "        ncols (int): Number of columns in the grid.\n",
    "        figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes = axes.flatten()  # Flatten the 2D array of subplots into a 1D array\n",
    "\n",
    "    # Ensure the number of images matches the number of subplots\n",
    "    num_images = len(images)\n",
    "    if num_images < nrows * ncols:\n",
    "        print(f\"Warning: Only {num_images} images provided, but {nrows * ncols} subplots created.\")\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < num_images:\n",
    "            ax.imshow(images[i])\n",
    "            ax.axis('off')\n",
    "            if titles:\n",
    "                ax.set_title(titles[i])\n",
    "        else:\n",
    "            ax.axis('off')  # Hide empty subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#displyaing 2x5 grid for traning images\n",
    "train_images = [train_set[i][0] for i in range(10)] \n",
    "train_labels = [train_set.classes[train_set[i][1]] for i in range(10)]  # Corresponding labels\n",
    "show_images(train_images, titles=train_labels, nrows=2, ncols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItidIYjeoej1"
   },
   "outputs": [],
   "source": [
    "# displaying UNLABELED set images\n",
    "torch.manual_seed(42)\n",
    "unlabeled_images = [unlabeled_set[i][0] for i in range(10)]\n",
    "show_images(unlabeled_images, nrows=2, ncols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODcIDLi9rr6u"
   },
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Compute mean and standard deviation for a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: PyTorch dataset (e.g., STL10).\n",
    "    Returns:\n",
    "        mean (list): Mean for each channel (R, G, B).\n",
    "        std (list): Standard deviation for each channel (R, G, B).\n",
    "    \"\"\"\n",
    "    mean = torch.zeros(3)  # For R, G, B channels\n",
    "    std = torch.zeros(3)   # For R, G, B channels\n",
    "    total_pixels = 0\n",
    "\n",
    "    for image, _ in dataset:\n",
    "        image = transforms.ToTensor()(image)  # Convert PIL Image to tensor\n",
    "        image = image.view(3, -1)  # Flatten the image to [3, height*width]\n",
    "        mean += image.sum(dim=1)   # Sum of pixel values for each channel\n",
    "        std += (image ** 2).sum(dim=1)  # Sum of squared pixel values for each channel\n",
    "        total_pixels += image.size(1)\n",
    "\n",
    "    # Compute the mean and std across the entire dataset\n",
    "    mean /= total_pixels\n",
    "    std = torch.sqrt(std / total_pixels - (mean ** 2))\n",
    "\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "mean, std = compute_mean_std(train_set)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAQGKU2AB2VH"
   },
   "outputs": [],
   "source": [
    "class STL10Augmentation:\n",
    "    def __init__(self, image_size=96, mean=None, std=None):\n",
    "        \"\"\"\n",
    "        Augmentation pipeline for STL-10 transformations.\n",
    "        Args:\n",
    "            image_size (int): Size of the output image (default: 96x96).\n",
    "            mean (list): Mean for each channel (R, G, B).\n",
    "            std (list): Standard deviation for each channel (R, G, B).\n",
    "        \"\"\"\n",
    "        self.augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.3),  # Random horizontal flip\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # Color jitter\n",
    "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=23)], p=0.5),  # Gaussian blur\n",
    "            transforms.RandomGrayscale(p=0.2),  # Random grayscale\n",
    "            transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "            transforms.Normalize(mean=mean, std=std)  # Normalize\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"\n",
    "        Apply augmentations to the input image.\n",
    "        Args:\n",
    "            image (PIL.Image): Input image.\n",
    "        Returns:\n",
    "            torch.Tensor: Augmented and normalized image.\n",
    "        \"\"\"\n",
    "        return self.augment(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdgXC3dCjT7w"
   },
   "outputs": [],
   "source": [
    "augmentation = STL10Augmentation(mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdniVKJoL46d"
   },
   "source": [
    "### Defining the MoCo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRYNT8yzLWhS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHVnLvL5MCGO"
   },
   "outputs": [],
   "source": [
    "class MoCo(nn.Module):\n",
    "    def __init__(self, base_encoder, dim=128, queue_size=32768, momentum=0.99, temperature=0.07, mlp=False):\n",
    "        \"\"\"\n",
    "        MoCo model for contrastive learning.\n",
    "        Args:\n",
    "        base_encoder: Base Encoder Network (ResNet)\n",
    "        dim: Dimension of the output feature vector\n",
    "        k (int): Size of the queue (number of negative samples)\n",
    "        m (float): Momentum for updating the key encoder\n",
    "        T (float): Temperature for the contrastive loss\n",
    "        \"\"\"\n",
    "        super(MoCo, self).__init__()\n",
    "\n",
    "        self.queue_size = queue_size\n",
    "        self.momentum = momentum\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # creating the query and key encoder\n",
    "        self.encoder_query = base_encoder(num_classes=dim)\n",
    "        self.encoder_key = base_encoder(num_classes=dim)\n",
    "\n",
    "        #initializing key encoder with query encoder weights\n",
    "        for param_query, param_key in zip(self.encoder_query.parameters(), self.encoder_key.parameters()):\n",
    "            param_key.data.copy_(param_query.data)\n",
    "            param_key.requires_grad = False #freezing key encoder\n",
    "\n",
    "        #queue for negative samples\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, queue_size))\n",
    "        self.queue = F.normalize(self.queue, dim=0)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_query, param_key in zip(self.encoder_query.parameters(), self.encoder_key.parameters()):\n",
    "            param_key.data = param_key.data * self.momentum + param_query.data * (1.0 - self.momentum)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def dequeue_and_enqueue(self, keys):\n",
    "       batch_size = keys.shape[0]\n",
    "       ptr = int(self.queue_ptr)\n",
    "\n",
    "       # handling the case where batch_size doesn't divide evenly into self.K\n",
    "       remaining = self.queue_size - ptr\n",
    "       if remaining < batch_size:\n",
    "           self.queue[:, ptr:ptr + remaining] = keys[:remaining].T\n",
    "           self.queue[:, 0:batch_size - remaining] = keys[remaining:].T\n",
    "       else:\n",
    "           self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "\n",
    "       ptr = (ptr + batch_size) % self.queue_size\n",
    "       self.queue_ptr[0] = ptr\n",
    "\n",
    "    def forward(self, query_image, key_image):\n",
    "      \"\"\"forward pass for MoCo\"\"\"\n",
    "\n",
    "      #query feature\n",
    "      query_features = self.encoder_query(query_image)\n",
    "      query_features = F.normalize(query_features, dim=1)\n",
    "\n",
    "      #key features\n",
    "      with torch.no_grad():\n",
    "        self._momentum_update_key_encoder()\n",
    "        key_features = self.encoder_key(key_image)\n",
    "        key_features = F.normalize(key_features, dim=1)\n",
    "\n",
    "      #computing logits\n",
    "      logit_positive = torch.einsum(\"nc, nc->n\",[query_features, key_features]).unsqueeze(-1)\n",
    "      logit_negative = torch.einsum(\"nc,ck->nk\",[query_features, self.queue.clone().detach()])\n",
    "      logits = torch.cat([logit_positive, logit_negative],dim=1) /self.temperature #applying temperature\n",
    "\n",
    "      #labels: the first column (positive key) is the ground truth\n",
    "      labels = torch.zeros(logits.shape[0], dtype=torch.long)\n",
    "\n",
    "      #updating the queue\n",
    "      self.dequeue_and_enqueue(key_features)\n",
    "\n",
    "      return logits, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IztKSPRWRO4f"
   },
   "source": [
    "### Defining the Base Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYUfwVyjRRYF"
   },
   "outputs": [],
   "source": [
    "def resnet18(num_classes=128):\n",
    "  \"\"\"\n",
    "  creating a resnet18 model with custom output dimension\n",
    "  Args:\n",
    "  num_classes (int): Dimension of the output feature vector\n",
    "  \"\"\"\n",
    "  model = models.resnet18(weights=None)\n",
    "  model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLnlRlBqRwCP"
   },
   "source": [
    "### Defining the Contrastive Loss, InfoNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1Cdc7BKR1Mi"
   },
   "outputs": [],
   "source": [
    "class InfoNCE(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(InfoNCE, self).__init__()\n",
    "\n",
    "  def forward(self, logits, labels):\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NLAt9yRSTUS"
   },
   "source": [
    "### Training the MoCo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Xlv7b0DSWf7"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "momentum = 0.999\n",
    "temperature = 0.07\n",
    "queue_size = 65536\n",
    "\n",
    "#to apply two augmentation to the unlabeled set\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]  \n",
    "    query_images = torch.stack([augmentation(img) for img in images]) \n",
    "    key_images = torch.stack([augmentation(img) for img in images])  \n",
    "    return query_images, key_images\n",
    "\n",
    "#to apply augmentation to train_set returning labels too\n",
    "def collate_fn_training(batch):\n",
    "    images = [item[0] for item in batch]  \n",
    "    labels = [item[1] for item in batch]  \n",
    "    \n",
    "    img_tensor = torch.stack([augmentation(img) for img in images])  \n",
    "    labels = torch.tensor(labels) \n",
    "\n",
    "    return img_tensor, labels\n",
    "\n",
    "#creating the dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True,collate_fn=collate_fn_training, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_set, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "#Creating the MoCo Model\n",
    "model = MoCo(resnet18, dim=128, queue_size = queue_size, momentum = momentum, temperature = temperature)\n",
    "criterion = InfoNCE()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary \n",
    "#printing summary of the encoder_query\n",
    "print(\"Summary for encoder_query:\")\n",
    "summary(model.encoder_query, input_size=(3, 96, 96), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correctly applied\n",
    "for img, labels in train_loader:\n",
    "    print(f\"im shape: {img.shape}\") \n",
    "    print(f\"labels shape: {labels.shape}\")  # Should be [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOEohGEokEaY"
   },
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(w.numel() for w in model.parameters() if w.requires_grad)\n",
    "\n",
    "num_trainable_params = count_trainable_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary \n",
    "#printing summary of the encoder_query\n",
    "print(\"Summary for encoder_query:\")\n",
    "summary(model.encoder_query, input_size=(3, 96, 96), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kJgvB5oVeLZ"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "total_training_time = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  for query_images, key_images in tqdm(unlabeled_loader):\n",
    "    query_images = query_images\n",
    "    key_images = key_images #the same image for query and key (with different augmentation)\n",
    "\n",
    "    #forward pass\n",
    "    logits, labels = model(query_images, key_images)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    #backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "  \n",
    "  end_time = time.time()\n",
    "  epoch_time = end_time - start_time\n",
    "  total_training_time += epoch_time\n",
    "  print(f\"Epoch[{epoch +1} /{num_epochs}], Loss: {total_loss / len(unlabeled_loader)}\")\n",
    "\n",
    "#printing training time for unlabelled set on CPU\n",
    "hours, remainder = divmod(total_training_time, 3600)  \n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f\"Total training time: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH2sdbl7WIVB"
   },
   "source": [
    "### Evaluating the model with a Linear Classifier on the Labeled Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor_transform = transforms.ToTensor()\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: (torch.stack([to_tensor_transform(item[0]) for item in x]), torch.tensor([item[1] for item in x]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the query encoder\n",
    "for param in model.encoder_query.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "#creating a linear classifier\n",
    "linear_classifier = nn.Linear(128, 10)\n",
    "optimizer = Adam(linear_classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2YQWSNyWOCk"
   },
   "outputs": [],
   "source": [
    "total_training_time = 0\n",
    "\n",
    "for epoch in range(5):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  for im, labels in tqdm(train_loader):\n",
    "    im = im\n",
    "    labels = labels\n",
    "    #extract feature\n",
    "    with torch.no_grad():\n",
    "      features = model.encoder_query(im)\n",
    "    #forward pass\n",
    "    outputs = linear_classifier(features)\n",
    "    outputs = outputs.type(torch.float32)  \n",
    "    labels = labels.type(torch.long)\n",
    "    loss = criterion(outputs, labels)  #correct loss calculation\n",
    "\n",
    "    #backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "  end_time = time.time()\n",
    "  epoch_time = end_time - start_time\n",
    "  total_training_time += epoch_time\n",
    "\n",
    "  print(f\"Epoch [{epoch + 1}/5], Loss: {total_loss / len(train_loader):.3f}, Accuracy: {100 * correct / len(train_set)}\")\n",
    "\n",
    "print(f\"Total training time: {total_training_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StMehmYXXjvS"
   },
   "source": [
    "#### Evaluate on the Test Set and Print Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5ytF8AmXhj7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YWK9P9XXVgx"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "for im, labels in tqdm(test_loader):\n",
    "    im = im\n",
    "    labels = labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder_q(im)\n",
    "        outputs = linear_classifier(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    y_true.extend(labels.cpu().numpy())\n",
    "    y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred, classes=train_set.classes)\n",
    "print(classification_report(y_true, y_pred, target_names=train_set.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert total training time to hours, minutes, and seconds\n",
    "total_seconds = int(23542)\n",
    "hours, remainder = divmod(total_seconds, 3600)  # 1 hour = 3600 seconds\n",
    "minutes, seconds = divmod(remainder, 60)  # 1 minute = 60 seconds\n",
    "\n",
    "# Print the total training time in a human-readable format\n",
    "print(f\"Total training time: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
