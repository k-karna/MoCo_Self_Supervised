{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TOeCqEZRB1FF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "C5kdgLdsn4iz",
        "outputId": "4376e5b7-2d8e-4ecb-e018-af852a6464ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|â–Š         | 200M/2.64G [00:12<02:36, 15.6MB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bd03ac6f7db0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0munlabeled_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unlabeled'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/stl10.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, folds, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset not found or corrupted. You can use download=True to download it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/stl10.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" to \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Getting dataset\n",
        "\n",
        "root = \"./data\"\n",
        "train_set = STL10(root=root, split=\"train\", download=True, transform=None)\n",
        "unlabeled_set = STL10(root=root, split='unlabeled', download=True, transform=None)\n",
        "test_set = STL10(root=root, split=\"test\", download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MY9br-xoaqZ"
      },
      "outputs": [],
      "source": [
        "def show_images(images, titles=None, nrows=2, ncols=5, figsize=(12, 6)):\n",
        "    \"\"\"\n",
        "    Display a grid of images.\n",
        "\n",
        "    Args:\n",
        "        images (list): List of images to display.\n",
        "        titles (list): List of titles for each image.\n",
        "        nrows (int): Number of rows in the grid.\n",
        "        ncols (int): Number of columns in the grid.\n",
        "        figsize (tuple): Size of the figure.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "    axes = axes.flatten()  # Flatten the 2D array of subplots into a 1D array\n",
        "\n",
        "    # Ensure the number of images matches the number of subplots\n",
        "    num_images = len(images)\n",
        "    if num_images < nrows * ncols:\n",
        "        print(f\"Warning: Only {num_images} images provided, but {nrows * ncols} subplots created.\")\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i < num_images:\n",
        "            ax.imshow(images[i])\n",
        "            ax.axis('off')\n",
        "            if titles:\n",
        "                ax.set_title(titles[i])\n",
        "        else:\n",
        "            ax.axis('off')  # Hide empty subplots\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#displyaing 2x5 grid for traning images\n",
        "train_images = [train_set[i][0] for i in range(10)]  # First 10 images\n",
        "train_labels = [train_set.classes[train_set[i][1]] for i in range(10)]  # Corresponding labels\n",
        "show_images(train_images, titles=train_labels, nrows=2, ncols=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItidIYjeoej1"
      },
      "outputs": [],
      "source": [
        "# displaying UNLABELED set images\n",
        "torch.manual_seed(42)\n",
        "unlabeled_images = [unlabeled_set[i][0] for i in range(10)]  # First 10 images\n",
        "show_images(unlabeled_images, nrows=2, ncols=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODcIDLi9rr6u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def compute_mean_std(dataset):\n",
        "    \"\"\"\n",
        "    Compute mean and standard deviation for a dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: PyTorch dataset (e.g., STL10).\n",
        "    Returns:\n",
        "        mean (list): Mean for each channel (R, G, B).\n",
        "        std (list): Standard deviation for each channel (R, G, B).\n",
        "    \"\"\"\n",
        "    mean = torch.zeros(3)  # For R, G, B channels\n",
        "    std = torch.zeros(3)   # For R, G, B channels\n",
        "    total_pixels = 0\n",
        "\n",
        "    for image, _ in dataset:\n",
        "        image = transforms.ToTensor()(image)  # Convert PIL Image to tensor\n",
        "        image = image.view(3, -1)  # Flatten the image to [3, height*width]\n",
        "        mean += image.sum(dim=1)   # Sum of pixel values for each channel\n",
        "        std += (image ** 2).sum(dim=1)  # Sum of squared pixel values for each channel\n",
        "        total_pixels += image.size(1)\n",
        "\n",
        "    # Compute the mean and std across the entire dataset\n",
        "    mean /= total_pixels\n",
        "    std = torch.sqrt(std / total_pixels - (mean ** 2))\n",
        "\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "mean, std = compute_mean_std(train_set)\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Std: {std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAQGKU2AB2VH"
      },
      "outputs": [],
      "source": [
        "class STL10Augmentation:\n",
        "    def __init__(self, image_size=96, mean=None, std=None):\n",
        "        \"\"\"\n",
        "        Augmentation pipeline for STL-10 transformations.\n",
        "        Args:\n",
        "            image_size (int): Size of the output image (default: 96x96).\n",
        "            mean (list): Mean for each channel (R, G, B).\n",
        "            std (list): Standard deviation for each channel (R, G, B).\n",
        "        \"\"\"\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(image_size, scale=(0.2, 1.0)),  # Random crop and resize\n",
        "            transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  # Color jitter\n",
        "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=23)], p=0.5),  # Gaussian blur\n",
        "            transforms.RandomGrayscale(p=0.2),  # Random grayscale\n",
        "            transforms.ToTensor(),  # Convert PIL Image to tensor\n",
        "            transforms.Normalize(mean=mean, std=std)  # Normalize\n",
        "        ])\n",
        "\n",
        "    def __call__(self, image):\n",
        "        \"\"\"\n",
        "        Apply augmentations to the input image.\n",
        "        Args:\n",
        "            image (PIL.Image): Input image.\n",
        "        Returns:\n",
        "            torch.Tensor: Augmented and normalized image.\n",
        "        \"\"\"\n",
        "        return self.augment(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdgXC3dCjT7w"
      },
      "outputs": [],
      "source": [
        "augmentation = STL10Augmentation(mean=mean, std=std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdniVKJoL46d"
      },
      "source": [
        "### Defining the MoCo Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRYNT8yzLWhS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHVnLvL5MCGO"
      },
      "outputs": [],
      "source": [
        "class MoCo(nn.Module):\n",
        "    def __init__(self, base_encoder, dim=128, K=32768, m=0.99, T=0.07, mlp=False):\n",
        "        \"\"\"\n",
        "        MoCo model for contrastive learning.\n",
        "        Args:\n",
        "        base_encoder: Base Encoder Network (ResNet)\n",
        "        dim: Dimension of the output feature vector\n",
        "        k (int): Size of the queue (number of negative samples)\n",
        "        m (float): Momentum for updating the key encoder\n",
        "        T (float): Temperature for the contrastive loss\n",
        "        \"\"\"\n",
        "        super(MoCo, self).__init__()\n",
        "\n",
        "        self.K = K\n",
        "        self.m = m\n",
        "        self.T = T\n",
        "\n",
        "        # create the query and key encoder\n",
        "        self.encoder_q = base_encoder(num_classes=dim)\n",
        "        self.encoder_k = base_encoder(num_classes=dim)\n",
        "\n",
        "        #initializing key encoder with query encoder weights\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False #freezing key encoder\n",
        "\n",
        "        #queue for negative samples\n",
        "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
        "        self.queue = F.normalize(self.queue, dim=0)\n",
        "\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        \"\"\"\n",
        "        Momentum update of the key encoder\n",
        "        \"\"\"\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def dequeue_and_enqueue(self, keys):\n",
        "       batch_size = keys.shape[0]\n",
        "       ptr = int(self.queue_ptr)\n",
        "\n",
        "       # Handle the case where batch_size doesn't divide evenly into self.K\n",
        "       remaining = self.K - ptr\n",
        "       if remaining < batch_size:\n",
        "           self.queue[:, ptr:ptr + remaining] = keys[:remaining].T\n",
        "           self.queue[:, 0:batch_size - remaining] = keys[remaining:].T\n",
        "       else:\n",
        "           self.queue[:, ptr:ptr + batch_size] = keys.T\n",
        "\n",
        "       ptr = (ptr + batch_size) % self.K\n",
        "       self.queue_ptr[0] = ptr\n",
        "\n",
        "    def forward(self, im_q, im_k):\n",
        "      \"\"\"forward pass for MoCo\"\"\"\n",
        "\n",
        "      #query feature\n",
        "      q = self.encoder_q(im_q)\n",
        "      q = F.normalize(q, dim=1)\n",
        "\n",
        "      #key features\n",
        "      with torch.no_grad():\n",
        "        self._momentum_update_key_encoder()\n",
        "        k = self.encoder_k(im_k)\n",
        "        k = F.normalize(k, dim=1)\n",
        "\n",
        "      #compute logits\n",
        "      logit_pos = torch.einsum(\"nc, nc->n\",[q, k]).unsqueeze(-1)\n",
        "      logit_neg = torch.einsum(\"nc,ck->nk\",[q, self.queue.clone().detach()])\n",
        "      logits = torch.cat([logit_pos, logit_neg],dim=1) /self.T #applying temperature\n",
        "\n",
        "      #labels: the first column (positive key) is the ground truth\n",
        "      labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)\n",
        "\n",
        "      #update the queue\n",
        "      self.dequeue_and_enqueue(k)\n",
        "\n",
        "      return logits, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IztKSPRWRO4f"
      },
      "source": [
        "### Defining the Base Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYUfwVyjRRYF"
      },
      "outputs": [],
      "source": [
        "def resnet18(num_classes=128):\n",
        "  \"\"\"\n",
        "  creating a resnet18 model with custom output dimension\n",
        "  Args:\n",
        "  num_classes (int): Dimension of the output feature vector\n",
        "  \"\"\"\n",
        "  model = models.resnet18(weights=None)\n",
        "  model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLnlRlBqRwCP"
      },
      "source": [
        "### Defining the Contrastive Loss, InfoNCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1Cdc7BKR1Mi"
      },
      "outputs": [],
      "source": [
        "class InfoNCE(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(InfoNCE, self).__init__()\n",
        "\n",
        "  def forward(self, logits, labels):\n",
        "    return F.cross_entropy(logits, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NLAt9yRSTUS"
      },
      "source": [
        "### Training the MoCo Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xlv7b0DSWf7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 256\n",
        "num_epochs = 5\n",
        "learning_rate = 0.03\n",
        "momentum = 0.99\n",
        "temperature = 0.07\n",
        "queue_size = 32768\n",
        "\n",
        "# to_tensor_transform = transforms.ToTensor()\n",
        "\n",
        "# Step 8: Define the DataLoader with Two Augmented Views\n",
        "def collate_fn(batch):\n",
        "    images = [item[0] for item in batch]  # Extract PIL images from the batch\n",
        "    im_q = torch.stack([augmentation(img) for img in images])  # First augmented view\n",
        "    im_k = torch.stack([augmentation(img) for img in images])  # Second augmented view\n",
        "    return im_q, im_k\n",
        "\n",
        "#creating the dataloader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "unlabeled_loader = DataLoader(unlabeled_set, batch_size=batch_size, shuffle=True, num_workers=2,collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "#Creating the MoCo Model\n",
        "model = MoCo(resnet18, dim=128, K=queue_size, m =momentum, T=temperature)\n",
        "criterion = InfoNCE()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOEohGEokEaY"
      },
      "outputs": [],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_trainable_params = count_trainable_parameters(model)\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kJgvB5oVeLZ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for im_q, im_k in tqdm(unlabeled_loader):\n",
        "    im_q = im_q\n",
        "    im_k = im_k #the same image for query and key (with different augmentation)\n",
        "\n",
        "    #forward pass\n",
        "    logits, labels = model(im_q, im_k)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    #backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  print(f\"Epoch[{epoch +1} /{num_epochs}], Loss: {total_loss / len(unlabeled_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH2sdbl7WIVB"
      },
      "source": [
        "### Evaluating the model with a Linear Classifier on the Labeled Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2YQWSNyWOCk"
      },
      "outputs": [],
      "source": [
        "#freeze the query encoder\n",
        "for param in model.encoder_q.paramters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "#creating a linear classifier\n",
        "linear_classifier = nn.Linear(128, 10)\n",
        "optimizer = Adam(linear_classifier.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#evaluating loop\n",
        "\n",
        "for epoch in range(5):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  for im, labels in tqdm(train_loader):\n",
        "    im = im\n",
        "    labels = labels\n",
        "\n",
        "    #extract feature\n",
        "    with torch.no_grad():\n",
        "      features = model.encoder_q(im)\n",
        "    #forward pass\n",
        "    outputs = linear_classifier(features)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    #backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/5], Loss: {total_loss / len(train_loader)}, Accuracy: {100 * correct / len(train_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StMehmYXXjvS"
      },
      "source": [
        "#### Evaluate on the Test Set and Print Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5ytF8AmXhj7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YWK9P9XXVgx"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "model.eval()\n",
        "for im, labels in tqdm(test_loader):\n",
        "    im = im.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model.encoder_q(im)\n",
        "        outputs = linear_classifier(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    y_true.extend(labels.cpu().numpy())\n",
        "    y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, classes=train_set.classes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
